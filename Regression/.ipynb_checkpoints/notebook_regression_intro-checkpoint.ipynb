{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression for Learning Plattform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a wide class of models and a powerful tool when it comes to modeling different problems. \n",
    "\n",
    "The **basic task** is to fit a function, in the simplest case a line, through data. This simple method serves well to illustrate basic concepts, relevant for more complex models. The resulting model as a generalization of the data is used for interpretation and prediction of continuous values.  \n",
    "\n",
    "We will start with the simplest regression model, a **linear regression**. Linear regression is useful to:\n",
    "- Check the existence of a linear relationship between two or more variables\n",
    "- Quantify the strength of a given relationship\n",
    "- Grant insights via interpretation of coefficients\n",
    "- Make predictions for unknown values given a model and an input\n",
    "\n",
    "Regression is very often and widely used in practice due to its simplicity, interpretability, and relative robustness given the data as well as fulfillment of basic assumptions. Below, you will find several examples for applications:\n",
    "\n",
    "- Economics: Finding trends in time series\n",
    "- Medicine: Investigating treatment effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Use Case to Begin With"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will do first is a short example, to understand, what linear regression can be used for.  \n",
    "Let us look at a basic modeling problem of food quality.\n",
    "Imagine, an internet plattform having as information the average price (paid per person in Euro) and the corresponding average ranking of food quality from 1 to 5 stars (5 is the best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/food.png\" style=\"float: left;width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we definitely see here is that a higher price in general leads to a higher rating, which is also intuitive.  Now, given the data, we can quantify this relationship for the given data. At the end of this notebook, you should be able to answer following quesions more precisely with help of linear regression. For example:\n",
    "\n",
    "- How can we describe the given data with a simple mathematical formula (model) with two coefficients?\n",
    "- How well does the model describe the data?\n",
    "- If we have 30 Euros, what food quality can we approximately expect for this money?\n",
    "- What does investing an additional 5 Euros can bring with respect to food quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from This Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic understanding of the regression principle\n",
    "- Understanding of least squares estimation\n",
    "- Performing a simple regression in Python with two packages\n",
    "- Interpreting regression results via coefficients & fit statistics\n",
    "- Performing polynomial regression\n",
    "- Understanding the concept of bias-variance trade-off\n",
    "- Performing a residual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will do a more general example with some mathematical background, where we use only two variables, $X$ and $Y$. Feel free to think about what these could be in your context ;). we will work on a data set which we create by means of python functions. This data will help us to check later on, how good our regresson is performing as we know the true parameters of the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting the plot to standard parameters\n",
    "matplotlib.rcdefaults()\n",
    "# Formatting the size of future plots\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For random number generation\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "\n",
    "# For fitting of regression line \n",
    "\n",
    "# For univariate regression\n",
    "from scipy import stats\n",
    "\n",
    "# For multivariate regression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a small set of points, which were originally generated by sampling some values from a uniform distribution, adding some random noise and plotting the result. This is the **true linear relationship** we wish to reconstruct by means of the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reassures the reproducibility of the results\n",
    "\n",
    "# The number in brackets is arbitrary and just has to be the same\n",
    "seed(1)\n",
    "\n",
    "# Simulate 20 points in the interval 0 to 10\n",
    "number_points = 20\n",
    "\n",
    "# The multiplicator is used to stretch the scale but does not have any specific meaning\n",
    "# This holds also for all numbers, as an exercise the influence of it can be tested by changing it\n",
    "x = rand(number_points)*10\n",
    "\n",
    "# Define equation for the line and adding some noise with mu=0, sigma=5\n",
    "y = 3*x + 5 + np.random.normal(0, 5, size=number_points)\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(x, y, '.', markersize=10)\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.grid(False)\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we now want to do is to **fit a linear function** in the equation below. \n",
    "\n",
    "Thereby, X is the input value and Y is the output we wish to model. We cannot equate our model with the measured data, as there is still a random noise component leading to a distance between the line and each data point. \n",
    "\n",
    "This is an example of the **univariate** case, where only one input variable is given, a case with more variables is called **multivariate**, and the task generalizes to fitting a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y \\thickapprox \\beta_0 + \\beta_1X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Criterion - Residual Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple possibilities to draw a line through the sample data. The following plot visualizes some of them, which at a first glance might seem plausible. Therefore, an important step is the definition of the optimization criterion (goal), which defines the problem to be solved in our case and which defines the line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/optcriterion.png\" style=\"float: left;width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea is to fit a line closest to the data. So mathematically, the idea is to optimize the sum of distances to the line, measured in the $Y$ direction (ordinal regression). As the goal is to predict the $Y$ value, we measure the projection in its direction, which is visualized in the following figure. The distance of a single point to the line is called **residual**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/residual.png\" style=\"float: left; width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the second step, we take the squared residual. The squared approach has several reasons:\n",
    "1. Squaring cancels out the sign of the residual - the values do not cancel out if summed and are contribute equally for both directions.\n",
    "2. Strong deviations are penalized heavier. \n",
    "3. Optimization of a quadratic function is easier than of an absolute value (due to simpler derivatives). \n",
    "\n",
    "In the following, the formula of the residual sum of squares is displayed. Thereby, an individual residual $e_i$ of point $i$ is computed as the difference between the value $\\hat{y_i}$ and the real value $y_i$. Then the difference is squared and summed over all $n$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goal is to minimize the residual sum of squares, which means that the line lies closest to the data with respect to the predicted variable. The resulting optimization problem is solved for the parameters. \n",
    "\n",
    "There are several ways to optimize this function. The straight forward approach known from basic calculus is to take the first derivative of the function, set it to zero and then to solve for the parameters. In the multivariate case, it means computing the [gradient](https://en.wikipedia.org/wiki/Gradient) with respect to the parameters. Later on we will see examples, where this approach of analytically solving for the optimal parameters is no longer feasible. In such a case one would resort to using numerical optimization algorithms, such as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). In any case, a function minimization problem is to be solved, the method is flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Coefficients $\\beta_0$ and $\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derivate the coefficients as follows:\n",
    "1. Replace the estimated $\\hat{y_i}= \\beta_1x_i+\\beta_0$\n",
    "2. Compute the partial derivatives, set them to zero and solve the resulting system of equations for parameters.\n",
    "\n",
    "$$RSS= \\sum_{i=1}^n (y_i - (\\beta_1x_i+\\beta_0))^2$$\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\beta_0}= \\sum_{i=1}^n 2(y_i- (\\beta_0+ \\beta_1x_i))(-1) \\overset{!}{=} 0$$\n",
    "\n",
    "$$ n \\beta_0 = \\sum_{i=1}^n y_i - \\beta_1x_i$$\n",
    "\n",
    "3. Divide both sides by the number of points $n$ and substitute $\\frac{1}{n}\\sum_{i=1}^n y_i$ by the mean $\\bar{y}$, analogously for $\\frac{1}{n}\\sum_{i=1}^n x_i=\\bar{x}$.\n",
    "\n",
    "4. Rewriting the original equation of the linear regression leads to $\\bar{y} = \\beta_0 + \\beta_1\\bar{x}$. \n",
    "\n",
    "5. Use the reformulation of the latter and use the reformulation $\\beta_0 = \\bar{y}-\\beta_1\\bar{x}$ in the next steps. \n",
    "    \n",
    "$$ RSS= \\sum_{i=1}^n (y_i - (\\beta_1x_i+\\beta_0))^2 $$\n",
    "\n",
    "$$ RSS= \\sum_{i=1}^n (y_i - (\\beta_1x_i+\\bar{y}-\\beta_1\\bar{x}))^2 $$\n",
    "\n",
    "$$ RSS= \\sum_{i=1}^n (y_i - \\bar{y} - \\beta_1(x_i-\\bar{x}))^2 $$\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\beta_1}= \\sum_{i=1}^n 2(y_i - \\bar{y} - \\beta_1(x_i-\\bar{x}))(-1)(x_i - \\bar{x}) \\overset{!}{=} 0$$\n",
    "\n",
    "$$\\beta_1 = \\frac{ \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic visualization of the univariate problem is below and shows the basic idea to find the minimum of the RSS function. In the figure, the values of $\\beta_0$ and $\\beta_1$ correspond to the minimum of the RSS function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/rss.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit - $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, statistical libraries provide the $R^2$ statistic to evaluate the fit quality of a linear regression.\n",
    "In order to derive the goodness of fit coefficient $R^2$ mathematically, we introduce the definition of the variance of the dependent variable $Y$ first. \n",
    "\n",
    "The **variance** measures the squared distance of the fit to the mean $\\bar{y}$ of the variable $Y$. This corresponds to a regression with only the intercept. \n",
    "\n",
    "So the variance of $Y$ summarizes the total variance of the dependent variable, also called total sum of squares (TSS). The overall goal is to reduce the variance by introducing the model, e.g. a linear model:\n",
    "\n",
    "$$Var(Y)=\\frac{1 }{n}\\sum_{i=1}^N (y_i-\\bar{y})(y_i-\\bar{y})= TSS$$\n",
    "where \n",
    "$$\\bar{y}=\\frac{1}{n}\\sum_{i=1}^N y_i$$\n",
    "\n",
    "The following figure illustrates the result. \n",
    "\n",
    "In the left figure a stronger noise was added, therefore there is a higher **TSS** - more variance of $Y$ to explain. \n",
    "\n",
    "At the same time, the **RSS**, the residual sum of squares, is higher in the left figure - you can observe that the line is overall \"further away\" from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/tss.png\" style=\" width: 650px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ measures the quotient of the unexplained variance (TSS and RSS in the numerator) normalized by the total variance of the dependent variable (TSS in the denominator): \n",
    "\n",
    "$$R^2 = \\frac{TSS-RSS}{TSS}$$\n",
    "\n",
    "As we can see from the formula, the value of $R^2$ ranges from 0 to 1. \n",
    "\n",
    "The optimal case $R^2=1$ occurs when all the points lie exactly on the regression line. In this case, the resulting error measured by the sum of squared residuals is $RSS=0$, leading to $R^2 = (TSS - 0)/TSS = 1$.\n",
    "\n",
    "In the figure above, we see that $R^2$ is higher in the left plot, where about 91 % of the total variance of $Y$ is explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have seen, how the coefficient estimation works analytically, we will look at the implementation. There are several ways to implement linear regression in Python (eight of them are shown [here](https://medium.freecodecamp.org/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b)). In the following, we will exercise two of them. The first package *stats* implements the functionality of the univariate regression and returns the essential parameters as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line computes the relevant parameters of the regression\n",
    "# The left sides creates variables to receive returned values\n",
    "# The right side is given the input and output variables\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the result leads to the code and the figure before. It is always a good **plausibility check** to visually look at the result, a further check using the residuals will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result leads to the following\n",
    "# Here the original data used for regression\n",
    "plt.plot(x, y, 'o', label='Original data')\n",
    "#Using the slope and the intercept estimates the resulting linear function is plotted as follows\n",
    "plt.plot(x, intercept + slope*(x), 'r', label='Fitted line')\n",
    "\n",
    "# Configuring the plot\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplary Interpretation of the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the parameters from above teaches us the following:\n",
    "\n",
    "- **Slope**: the relationship between independent and dependent variable, where an increase of 1 in the independent variable $X$ corresponds to the increase in the dependent variable $Y$ of the corresponding slope\n",
    "- **Intercept**: given the value 0 of the independent variable, we get the corresponding value of the dependent variable\n",
    "- **$R^2$ (R-Squared)**: shows how well the line explains the data in the range from 0 to 1 as goodness of fit\n",
    "- **Standard error**: average deviation from the computed parameter \n",
    "- **P-value**: the statistical significance level - how sure are we about the estimated coefficients\n",
    "\n",
    "The next code block prints the coefficients, which are shortly interpreted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"slope:\", slope)\n",
    "print(\"intercept:\", intercept)\n",
    "print(\"r-squared:\", r_value**2)\n",
    "print(\"std_err:\", std_err)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The slope of 2.87 is very close to the original one of value 3.00, so given only 20 points we are quite close to the true value despite of noise with a standard deviation of 5.0 (see data generation to find the respective parameters and to change these to see the effect). \n",
    "- The intercept is even closer to the true parameter of 5.0. \n",
    "- The value of $R^2$ is close to 1 (maximal possible value), which says that 77 % of the variance in the Y direction are explained by the line. \n",
    "- The standard error computes the residual sum of squares normalized by a factor, containing the number of observations reduced by number of predictors, and therefore can be interpreted as the average squared error per observation (here with the 0.36 well below the standard deviation of the noise specified upfront). \n",
    "- The p-value is below the level of 0.001 (strong significance), which means that we can be statistically sure about the estimated slope parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit using Statmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fitting a multivariate regression and getting a more detailed result, another package is more suitable. The exemplary code for the same regression problem is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas data frame to be processed by the package\n",
    "df = pd.DataFrame({ 'x': x, 'y': y})\n",
    "\n",
    "# Creating the result object carrying all the information\n",
    "# Specifying the equation to be estimated\n",
    "results = smf.ols('y ~ x', data=df).fit()\n",
    "\n",
    "# Printing the result\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result, we see the same coefficients for the slope and intercept, and some additional information, which, however, we do not discuss here in detail. For details, please see the documentation and the referenced literature at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily generalize the fit of a linear function to other functions, e.g. quadratic or polynomials of a higher degrees.\n",
    "The idea remains the same - fitting a function closest to the data given the residual sum of squares as optimization target, but of the following form, where $d$ is the polynomial degree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\beta_dX^d + \\dots + \\beta_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same data, we now try to fit a polynomial function to the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a polynom of the tenth degree with least squares\n",
    "z = np.polyfit(x, y, 10)\n",
    "\n",
    "# Poly1d creates objects for handling polynoms\n",
    "# Encodes the coefficients or roots, if parameter r ist set to True\n",
    "z_equation = np.poly1d(z)\n",
    "\n",
    "# The following code prints the estimated equation\n",
    "print('Resulting Equation: \\n',z_equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting routine\n",
    "\n",
    "# Define a vector of size 100 in the interval from 0 to 9\n",
    "xp = np.linspace(0, 9, 100)\n",
    "\n",
    "# Plot the original points and the estimated line as a polynomial of the input space\n",
    "plt.plot(x, y, 'o',  xp, z_equation(xp), 'r-')\n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.title('Polynomial Function')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, there are multiple possibilities for fitting a model of varying complexity. The complexity of a model can be quantified by many metrics, one of the simplest and usually a good choice are the number of parameters a model uses. With respect to complexity, linear regression is then the simplest. The higher the degree of the polynomial, the higher the resulting complexity. Therefore, the question of the right model complexity arises. The answer is provided by the so-called **bias-variance trade-off**.\n",
    "\n",
    "In this notebook, we discuss only the general idea of this concept. For more detailed information please refer to the book [Introduction to Statistical Learning](https://inside-docupedia.bosch.com/confluence/display/EAIAB/Introduction+to+Statistical+Learning), which explains this topic very well.\n",
    "\n",
    "The following figure illustrates the trade-off. The **bias** describes the loss of information due to the lacking complexity of the model. An example would be fitting a line through quadratic data. So the simpler the model, the higher the bias.\n",
    "\n",
    "The second component is the **variance**, which rises with rising complexity. Variance in this case refers to the variance of the estimated parameters, meaning that given a small change in the data, a strong change in the parameter estimate arises. As a result, every point in the data influences the fitted function. Consequently, the model does not generalize well, as can be seen in the right figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/bv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both components are interchangeable, as each reduction of one of them leads to an increase of the other, which is shown in the next figure. The total error of a model is then the sum of both components plus an irreducible random error which is not discussed here. Overall, it is possible to find an optimum for each data set with respect to model complexity, which is in our case the degree of the polynomial. A more general description is the number of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/to.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis - Checking the Linearity Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear function might not the best idea. As the following example will show, sometimes we need another solution. Now, we will generate some data using a quadratic function and see how we can assess the modeling result, not only for linear regression. Namely, we will do it by means of the residual analysis, which is a part of the general approach for **regression diagnostics**. \n",
    "\n",
    "Let us generate some data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "# Generate 100 equally spaced points between -10 and 10 as input variable\n",
    "x = np.linspace(-3, 3, 100)\n",
    "# Generate random noise vector of the same length\n",
    "nse = np.random.normal(size=100)\n",
    "# Calculate the y value for each element of the x vector\n",
    "y = x**2 + 2*x + nse \n",
    "\n",
    "plt.scatter(x, y) \n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit a linear function through the data as done before, and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the resulting parameters\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "\n",
    "# Plotting the result leads to the following\n",
    "plt.plot(x, y, 'o', label='Original data')\n",
    "plt.plot(x, intercept + slope*(x), 'r', label='Fitted line')\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the plot, we immediately see that a line is a misfit in this case. Nevertheless, by evaluating the $R^2$ of the model, we get a result of 0.52, meaning that even a suboptimal model explains more than 50% of the output variable. Therefore, a high $R^2$ does not necessary mean that the model is optimal to the data. So the question is how else we can assess the modeling result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the resulting estimates\n",
    "print(\"intercept:\", intercept)\n",
    "print(\"slope:\", slope)\n",
    "print(\"r-squared:\", r_value**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the plot above, fitting a line is obviously not optimal. Besides visual analysis, which immediately shows the resulting problem, we will get to know a very simple but a very powerful idea from **residual analysis**. The core idea is to plot the residuals, i.e. the distance to the regression line, but unsquared as it keeps the direction of the error.\n",
    "\n",
    "The residuals are plotted against the predicted value of the output variable $Y$. Therefore, this approach can be used for multivariate cases, where visual plot assessment per dimension gets more difficult. There is a very simple rule: As long there is a pattern in the residuals, something is wrong. If a person recognizes a system behind the errors, there should be a way to explain it with a mathematical model. The question is then how to \"explain\" these systematics, which a person sees in the residual analysis, to the model with the goal to incorporate this information. \n",
    "\n",
    "The package *seaborn* delivers a very simple utility to visualize the residuals. It gets the predicted value as a scale for the x-axis and the true value of the output variable as y-axis. As a result, we see a clear pattern, which is due to the presence of a direction (+/-). In the center, we see a systematic overestimation. At the sides, we see a systematic underestimation. The green line represents a trend line, obtained by a locally weighted regression. The latter performs a linear regression on the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(intercept + slope*x, y, lowess=True, color=\"g\")\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.title(\"Residual Analysis\")\n",
    "plt.xlabel(\"Predicted Value Y\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we will raise the complexity of the model to reduce the bias (underfit of a too simple model). Therefore, we will fit a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code fits a quadratic function\n",
    "z = np.polyfit(x, y, 2)\n",
    "\n",
    "z_equation = np.poly1d(z)\n",
    "# Printing the equation\n",
    "print('Resulting Equation: \\n',z_equation)\n",
    "\n",
    "# Defining the x-values for plotting the function line\n",
    "xp = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Plotting the result leads to the following\n",
    "plt.plot(x, y, 'o', label='Original data')\n",
    "plt.plot( xp, z_equation(xp), 'r', label='Fitted line')\n",
    "plt.title(\"Polynomial Regression\")\n",
    "plt.xlabel(\"Value X\")\n",
    "plt.ylabel(\"Value Y\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we see, that the fit is much better, even visually. In the next code, we reevaluate the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the parameter \"order\", the function automatically fits a second \n",
    "# degree polynom and computes the corresponding residuals\n",
    "sns.residplot(intercept + slope*x,y, lowess=True, color=\"g\", order = 2)\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.title(\"Residual Analysis\")\n",
    "plt.xlabel(\"Predicted Value Y\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the residual plot shows no pattern, which is the perfect picture of the residuals. The residuals are distributed randomly and the trend line does not expose any pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions You Should Be Able to Answer Now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have carefully read and understood the contents of this notebook, you should be able to answer following questions.\n",
    "\n",
    "- Which optimization criterion does the least squares approach have? \n",
    "- Name some of the packages and functions, that implement linear regression in Python.\n",
    "- What is the interpretation of the following parameters?\n",
    "    - Slope\n",
    "    - Intercept \n",
    "    - R-Squared\n",
    "- What is the difference between the residual sum of squares (RSS) and the total sum of squares (TSS)?\n",
    "- What does the bias-variance trade-off state? How would you explain bias and variance with respect to model complexity?\n",
    "- What is the main rule if performing residual analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Topics and Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should serve as an initial read for understanding the principle and basic application of the linear and polynomial regression in Python. The following list of related topics and keywords is a good way to proceed.\n",
    "\n",
    "- Basic assumptions of linear regression\n",
    "- Adjusted R-Squared\n",
    "- Confidence and prediction intervals\n",
    "- Regression diagnostics\n",
    "- Feature selection\n",
    "- Ridge and lasso regression\n",
    "- Bayesian linear regression\n",
    "- Logistic regression\n",
    "\n",
    "As a general overview, we advise following literature:\n",
    "\n",
    "- [Introduction to Statistical Learning](https://inside-docupedia.bosch.com/confluence/display/EAIAB/Introduction+to+Statistical+Learning)\n",
    "- [Further ressources on regression](https://inside-docupedia.bosch.com/confluence/display/EAIAB/Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task for You to Perform on Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to provide you with a task to perform on your own in order to apply the knowledge presented before.\n",
    "The data set from the [UCI repository](http://mlr.cs.umass.edu/ml/datasets.html) summarizes some parameters of different car types. The data set and the list of attributes can be found [here](http://mlr.cs.umass.edu/ml/datasets/Auto+MPG).\n",
    "In the following, we will propose some tasks and reference some commands to perform these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands will load the data and perform an initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the car data\n",
    "# To read the file and to specify the delimiter parameter we do the following\n",
    "df1 = pd.read_csv('input/car_data.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to get to know the data with the listed commands.\n",
    "For you to learn better we provide only the link to the respective function documentation. If you have troubles, you can do one of the following: \n",
    "\n",
    "- Write a post in our community [here](https://connect.bosch.com/communities/service/html/communitystart?communityUuid=210bdd9f-74c5-4831-a5b5-618ad489f103)\n",
    "- The less preferred one: consult the solution code [here](assets/solution/regression_intro_solution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dimensionality of the data with shape\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first rows of the data \n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting descriptive statistics of the continuous variables\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chechikng if the data types are plausible\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dtypes.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"horsepower\" has the type object, which is a sign for it to contain different data types (e.g. missing values). We will try casting it to numeric, as the values are obviously numbers (see *head* command and its result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting horsepower to numeric with the following function\n",
    "# Use downcast and errors parameters with appropriate values\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result with an appropriate command\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for missing values with dropna\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result with an appropriate command\n",
    "# How many rows had a missing value in horse power?\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For frequent numerical or categorical variables the function crosstab can be used, e.g. for cylinders\n",
    "# https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.crosstab.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a Box plot of a continuous variable, e.g. for the weight\n",
    "# https://matplotlib.org/gallery/pyplots/boxplot_demo_pyplot.html#sphx-glr-gallery-pyplots-boxplot-demo-pyplot-py\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a scatter plot of a variable pair, e.g. for the weight and miles per gallon (mpg)\n",
    "# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a pair plot for each variable pair or for the whole data set\n",
    "# https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a regression using the stats-syntax\n",
    "# Print and interpret the estimates\n",
    "# Use weight as input variable and miles per gallon (mpg) as output variable\n",
    "# See the example above or the documentation example at\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results using the example above or\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a regression using the statmodels-syntax\n",
    "# Use weight as input variable miles per gallon (mpg) as output variable\n",
    "# See the example above or the documentation example (alternative syntax) at\n",
    "# https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a residual analysis using the example above or \n",
    "# https://seaborn.pydata.org/examples/residplot.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial model for the same variables using the example above or\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a residual analysis using the example above or \n",
    "# https://seaborn.pydata.org/examples/residplot.html\n",
    "# Use the order parameter to adkust the polynomial degree\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this notebook are a basic introductory part from our class room training [Basics of Machine Learning](https://inside-docupedia.bosch.com/confluence/display/EAIAB/Basics+of+Machine+Learning). If you are interested to know more, please visit the training website and contact us for further information using the envelope button at the top of your screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel Free to Consult our [Training Bar](https://inside-docupedia.bosch.com/confluence/display/EAIAB/Training+Offers) for Further Topics and Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention regarding external links**: You are forwarded to an external provider who is not related to us. Upon clicking on the link, we have no influence on the collecting, processing and use of personal data possibly transmitted by clicking on the link to the third party (such as the IP address or the URL of the site on which the link is located) as the conduct of third parties is naturally beyond our control. We do not assume responsibility for the processing of personal data by third parties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
