<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Evaluation of clustering</title>
<meta name="description" content="Evaluation of clustering">
<meta name="keywords" content="irbook">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">

<meta name="Generator" content="LaTeX2HTML v2002-2-1">
<meta http-equiv="Content-Style-Type" content="text/css">

<link rel="STYLESHEET" href="Evaluation%20of%20clustering_files/irbook.htm">

<link rel="next" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">
<link rel="previous" href="https://nlp.stanford.edu/IR-book/html/htmledition/problem-statement-1.html">
<link rel="up" href="https://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html">
<link rel="next" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">
</head>

<body>
<!--Navigation Panel-->
<a name="tex2html4211" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">
<img alt="next" src="Evaluation%20of%20clustering_files/next.png" width="37" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4205" href="https://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html">
<img alt="up" src="Evaluation%20of%20clustering_files/up.png" width="26" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4199" href="https://nlp.stanford.edu/IR-book/html/htmledition/cardinality---the-number-of-clusters-1.html">
<img alt="previous" src="Evaluation%20of%20clustering_files/prev.png" width="63" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4207" href="https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html">
<img alt="contents" src="Evaluation%20of%20clustering_files/contents.png" width="65" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4209" href="https://nlp.stanford.edu/IR-book/html/htmledition/index-1.html">
<img alt="index" src="Evaluation%20of%20clustering_files/index.png" width="43" height="24" border="0" align="BOTTOM"></a> 
<br>
<b> Next:</b> <a name="tex2html4212" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">K-means</a>
<b> Up:</b> <a name="tex2html4206" href="https://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html">Flat clustering</a>
<b> Previous:</b> <a name="tex2html4200" href="https://nlp.stanford.edu/IR-book/html/htmledition/cardinality---the-number-of-clusters-1.html">Cardinality - the number</a>
 &nbsp; <b>  <a name="tex2html4208" href="https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html">Contents</a></b> 
 &nbsp; <b>  <a name="tex2html4210" href="https://nlp.stanford.edu/IR-book/html/htmledition/index-1.html">Index</a></b> 
<br>
<br>
<!--End of Navigation Panel-->

<h1><a name="SECTION002130000000000000000"></a>
<a name="sec:clustereval"></a> <a name="p:clustereval"></a>
<br>
Evaluation of clustering
</h1> 

<p>
Typical objective functions in clustering formalize the goal
of attaining high intra-cluster similarity (documents within
a cluster are similar) and low inter-cluster similarity
(documents from different clusters are dissimilar). This is
an <a name="24251"></a> <i>internal criterion</i>  for the quality of a
clustering. But good scores on an internal criterion do not
necessarily translate into good effectiveness in an
application. An alternative to internal criteria is direct
evaluation in the application of interest. For search result
clustering, we may want to measure the time it takes users
to find an answer with different clustering algorithms. This
is the most direct evaluation, but it is expensive,
especially if large user studies are necessary.

</p><p>
As a surrogate for user judgments, we can use a set of classes
in an
evaluation benchmark or gold standard
(see Section <a href="https://nlp.stanford.edu/IR-book/html/htmledition/assessing-relevance-1.html#sec:test-collections">8.5</a> ,
page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/assessing-relevance-1.html#p:test-collections">8.5</a> , and Section <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-text-classification-1.html#sec:evalclass">13.6</a> ,
page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-text-classification-1.html#p:evalclass">13.6</a> ).
The gold
standard is ideally produced by human judges with a good
level of inter-judge agreement
(see Chapter <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-in-information-retrieval-1.html#ch:evaluation">8</a> , page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/information-retrieval-system-evaluation-1.html#p:goldstandard">8.1</a> ).
We can then
compute an <a name="24259"></a> <i>external criterion</i> 
that evaluates
how well the clustering matches the gold standard classes.
For example, we may want to say
that the optimal
clustering of the search results for jaguar in
Figure <a href="https://nlp.stanford.edu/IR-book/html/htmledition/clustering-in-information-retrieval-1.html#fig:clustfg1">16.2</a>  consists of three classes
corresponding to the three senses car, animal, and
operating system.  In this type of evaluation, we
only use the partition provided by the gold standard, not
the class labels.

</p><p>
This section introduces four external criteria of clustering
quality.  <i>Purity</i> is a simple and transparent
evaluation measure. <i>Normalized mutual information</i> can be
information-theoretically 
interpreted.  The <i>Rand
index</i> penalizes both false positive and false negative
decisions during clustering.  The <i>F&nbsp;measure</i> in
addition supports differential weighting of these two types
of errors.

</p><p>

</p><div align="CENTER">

<p><a name="fig:clustfg3"></a><a name="p:clustfg3"></a></p><img src="Evaluation%20of%20clustering_files/img1393.png" alt="\begin{figure}
% latex2html id marker 24270
\par
\psset{unit=0.75cm}
\par
\begin...
...$,
3
(cluster 3).
Purity is $(1/17) \times (5+4+3)
\approx 0.71$.
}
\end{figure}" width="556" height="231" border="0">
</div>

<p>
To compute <a name="24299"></a> <i>purity</i> ,
each cluster is assigned to the class which is most
frequent in the cluster, and then the accuracy of this
assignment is measured by counting the number of correctly
assigned documents and dividing by <img src="Evaluation%20of%20clustering_files/img62.png" alt="$N$" width="17" height="32" border="0" align="MIDDLE">. Formally:
<br>
</p><div align="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{purity}(
\Omega,\mathbb{C}
) = 
\frac{1}{N}
\sum_k \max_j
|\omega_k \cap
c_j|
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="CENTER"><a name="eqn:purity"></a><img src="Evaluation%20of%20clustering_files/img1394.png" alt="\begin{displaymath}
\mbox{purity}(
\Omega,\mathbb{C}
) =
\frac{1}{N}
\sum_k \max_j
\vert\omega_k \cap
c_j\vert
\end{displaymath}" width="242" height="48" border="0"></td>
<td width="10" align="RIGHT">
(182)</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>
where
<!-- MATH
 $\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$
 -->
<img src="Evaluation%20of%20clustering_files/img1395.png" alt="$\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$" width="158" height="33" border="0" align="MIDDLE"><a name="Omega-notation"></a><a name="omega-notation"></a>is the set of clusters and
<!-- MATH
 $\mathbb{C} = \{ c_1, c_2, \ldots,
c_J \}$
 -->
<img src="Evaluation%20of%20clustering_files/img855.png" alt="$\mathbb{C} = \{ c_1,c_2,\ldots,c_J \}$" width="137" height="33" border="0" align="MIDDLE"> is the set of classes. We interpret
<img src="Evaluation%20of%20clustering_files/img1396.png" alt="$\omega_k$" width="22" height="32" border="0" align="MIDDLE"> as the set of documents in 
<img src="Evaluation%20of%20clustering_files/img1396.png" alt="$\omega_k$" width="22" height="32" border="0" align="MIDDLE"> and
<img src="Evaluation%20of%20clustering_files/img1191.png" alt="$c_j$" width="16" height="32" border="0" align="MIDDLE"> as the set
of documents in <img src="Evaluation%20of%20clustering_files/img1191.png" alt="$c_j$" width="16" height="32" border="0" align="MIDDLE"> in Equation&nbsp;<a href="#eqn:purity">182</a>.

<p>
We present an example of how to compute purity in
Figure <a href="#fig:clustfg3">16.4</a> .<a name="tex2html182" href="https://nlp.stanford.edu/IR-book/html/htmledition/footnode.html#foot25225"><sup><img alt="[*]" src="Evaluation%20of%20clustering_files/footnote.png" border="1" align="BOTTOM"></sup></a> Bad
clusterings have purity values close to 0, a perfect
clustering has a purity of  1 . Purity is compared with the other three
measures discussed in this chapter in Table <a href="#tab:clmeascomp">16.2</a> .

</p><p>
<br></p><p></p>
<div align="CENTER"><a name="25226"></a>
<table>
<caption><strong>Table 16.2:</strong>
The four external evaluation measures applied to
the clustering in Figure <a href="#fig:clustfg3">16.4</a> .</caption>
<tbody><tr><td><table cellpadding="3" border="1">
<tbody><tr><td align="LEFT">&nbsp;</td>
<td align="LEFT">purity</td>
<td align="LEFT">NMI</td>
<td align="LEFT">RI</td>
<td align="LEFT"><img src="Evaluation%20of%20clustering_files/img1397.png" alt="$F_5$" width="19" height="32" border="0" align="MIDDLE"></td>
</tr>
<tr><td align="LEFT">lower bound</td>
<td align="LEFT">0.0</td>
<td align="LEFT">0.0</td>
<td align="LEFT">0.0</td>
<td align="LEFT">0.0</td>
</tr>
<tr><td align="LEFT">maximum</td>
<td align="LEFT">1</td>
<td align="LEFT">1</td>
<td align="LEFT">1</td>
<td align="LEFT">1</td>
</tr>
<tr><td align="LEFT">value for Figure <a href="#fig:clustfg3">16.4</a></td>
<td align="LEFT">0.71</td>
<td align="LEFT">0.36</td>
<td align="LEFT">0.68</td>
<td align="LEFT">0.46</td>
</tr>
</tbody></table>

<a name="tab:clmeascomp"></a> <a name="p:clmeascomp"></a> 
</td></tr>
</tbody></table>
</div><p></p>
<br>

<p>
High purity is easy to achieve when the number of
clusters is large - in particular, purity is  1  if each document gets
its own cluster. 
Thus, we cannot use purity to
trade off 
the quality of the clustering against the
number of clusters.

</p><p>
A measure that allows us to make this tradeoff is
<a name="24325"></a> <i>normalized mutual
information</i>  or <a name="24327"></a> <i>NMI</i> :
<br>
</p><div align="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{NMI}(\Omega , \mathbb{C})
= 
\frac{
I(\Omega ; \mathbb{C})
}
{
[H(\Omega)+ H(\mathbb{C} )]/2
}
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="CENTER"><a name="nmidef"></a><img src="Evaluation%20of%20clustering_files/img1398.png" alt="\begin{displaymath}
\mbox{NMI}(\Omega , \mathbb{C})
=
\frac{
I(\Omega ; \mathbb{C})
}
{
[H(\Omega)+ H(\mathbb{C} )]/2
}
\end{displaymath}" width="232" height="45" border="0"></td>
<td width="10" align="RIGHT">
(183)</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>
<img src="Evaluation%20of%20clustering_files/img1399.png" alt="$I$" width="11" height="32" border="0" align="MIDDLE"> is <a name="24336"></a>mutual information (cf. Chapter <a href="https://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html#ch:nbayes">13</a> ,
page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html#p:mutualinfo">13.5.1</a> ):
<br>
<div align="CENTER"><a name="midef2"></a><a name="midef2ml"></a>
<!-- MATH
 \begin{eqnarray}
I( \Omega ; \mathbb{C} )
&=&
\sum_k \sum_j     P(\omega_k \cap c_j) \log
\frac{P(\omega_k \cap c_j)}{P(\omega_k)P(c_j)}\\
&=&
\sum_k \sum_j     \frac{|\omega_k \cap c_j|}{N} \log
\frac{N|\omega_k \cap c_j|}{|\omega_k||c_j|}
\end{eqnarray}
 -->
<table width="100%" cellpadding="0" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="RIGHT"><img src="Evaluation%20of%20clustering_files/img1400.png" alt="$\displaystyle I( \Omega ; \mathbb{C} )$" width="56" height="33" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img313.png" alt="$\textstyle =$" width="17" height="32" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="LEFT"><img src="Evaluation%20of%20clustering_files/img1401.png" alt="$\displaystyle \sum_k \sum_j P(\omega_k \cap c_j) \log
\frac{P(\omega_k \cap c_j)}{P(\omega_k)P(c_j)}$" width="226" height="60" border="0" align="MIDDLE"></td>
<td width="10" align="RIGHT">
(184)</td></tr>
<tr valign="MIDDLE"><td nowrap="nowrap" align="RIGHT">&nbsp;</td>
<td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img313.png" alt="$\textstyle =$" width="17" height="32" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="LEFT"><img src="Evaluation%20of%20clustering_files/img1402.png" alt="$\displaystyle \sum_k \sum_j \frac{\vert\omega_k \cap c_j\vert}{N} \log
\frac{N\vert\omega_k \cap c_j\vert}{\vert\omega_k\vert\vert c_j\vert}$" width="209" height="60" border="0" align="MIDDLE"></td>
<td width="10" align="RIGHT">
(185)</td></tr>
</tbody></table></div>
<br clear="ALL"><p></p>
where 
<img src="Evaluation%20of%20clustering_files/img1403.png" alt="$P(\omega_k)$" width="47" height="33" border="0" align="MIDDLE">, <img src="Evaluation%20of%20clustering_files/img1404.png" alt="$P(c_j)$" width="39" height="33" border="0" align="MIDDLE">, and <!-- MATH
 $P(\omega_k
\cap c_j)$
 -->
<img src="Evaluation%20of%20clustering_files/img1405.png" alt="$P(\omega_k
\cap c_j)$" width="76" height="33" border="0" align="MIDDLE"> are the
probabilities of a document being
in cluster <img src="Evaluation%20of%20clustering_files/img1396.png" alt="$\omega_k$" width="22" height="32" border="0" align="MIDDLE">, class <img src="Evaluation%20of%20clustering_files/img1191.png" alt="$c_j$" width="16" height="32" border="0" align="MIDDLE">, and in the intersection of <img src="Evaluation%20of%20clustering_files/img1396.png" alt="$\omega_k$" width="22" height="32" border="0" align="MIDDLE">
and <img src="Evaluation%20of%20clustering_files/img1191.png" alt="$c_j$" width="16" height="32" border="0" align="MIDDLE">, respectively. 
Equation <a href="#midef2ml">185</a>  is equivalent to
Equation&nbsp;<a href="#midef2">184</a> for maximum likelihood estimates of the
probabilities (i.e., the estimate of each probability is the
corresponding relative frequency).

<p>
<img src="Evaluation%20of%20clustering_files/img317.png" alt="$H$" width="18" height="32" border="0" align="MIDDLE"> is <a name="24352"></a>entropy as defined in Chapter <a href="https://nlp.stanford.edu/IR-book/html/htmledition/index-compression-1.html#ch:icompress">5</a> 
(page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/gamma-codes-1.html#p:entropy">5.3.2</a> ):
<br>
</p><div align="CENTER">

<!-- MATH
 \begin{eqnarray}
H(\Omega) &=& -\sum_k P(\omega_k) \log P(\omega_k)\\
&=& -\sum_k \frac{|\omega_k|}{N} \log \frac{|\omega_k|}{N}
\end{eqnarray}
 -->
<table width="100%" cellpadding="0" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="RIGHT"><img src="Evaluation%20of%20clustering_files/img1406.png" alt="$\displaystyle H(\Omega)$" width="45" height="33" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img313.png" alt="$\textstyle =$" width="17" height="32" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="LEFT"><img src="Evaluation%20of%20clustering_files/img1407.png" alt="$\displaystyle -\sum_k P(\omega_k) \log P(\omega_k)$" width="153" height="48" border="0" align="MIDDLE"></td>
<td width="10" align="RIGHT">
(186)</td></tr>
<tr valign="MIDDLE"><td nowrap="nowrap" align="RIGHT">&nbsp;</td>
<td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img313.png" alt="$\textstyle =$" width="17" height="32" border="0" align="MIDDLE"></td>
<td nowrap="nowrap" align="LEFT"><img src="Evaluation%20of%20clustering_files/img1408.png" alt="$\displaystyle -\sum_k \frac{\vert\omega_k\vert}{N} \log \frac{\vert\omega_k\vert}{N}$" width="132" height="56" border="0" align="MIDDLE"></td>
<td width="10" align="RIGHT">
(187)</td></tr>
</tbody></table></div>
<br clear="ALL"><p></p>
where, again, the second equation is based on maximum
likelihood estimates of the probabilities.

<p>
<!-- MATH
 $I( \Omega ; \mathbb{C} )$
 -->
<img src="Evaluation%20of%20clustering_files/img1409.png" alt="$I( \Omega ; \mathbb{C} )$" width="57" height="33" border="0" align="MIDDLE"> in Equation&nbsp;<a href="#midef2">184</a> measures the
amount of information by which our knowledge about the
classes increases when we are told what the clusters are.
The minimum of <!-- MATH
 $I( \Omega ; \mathbb{C} )$
 -->
<img src="Evaluation%20of%20clustering_files/img1409.png" alt="$I( \Omega ; \mathbb{C} )$" width="57" height="33" border="0" align="MIDDLE"> is 0 if the
clustering is random with respect to class membership. In that
case, knowing that a document is in a particular cluster
does not give us any new information about what its class
might be. Maximum mutual information is reached for a
clustering <!-- MATH
 $\Omega_{exact}$
 -->
<img src="Evaluation%20of%20clustering_files/img1410.png" alt="$\Omega_{exact}$" width="45" height="32" border="0" align="MIDDLE"> that perfectly recreates the
classes - but also if clusters in <!-- MATH
 $\Omega_{exact}$
 -->
<img src="Evaluation%20of%20clustering_files/img1410.png" alt="$\Omega_{exact}$" width="45" height="32" border="0" align="MIDDLE"> are
further subdivided into smaller clusters
(Exercise <a href="https://nlp.stanford.edu/IR-book/html/htmledition/exercises-3.html#ex:miclustering">16.7</a> ).  In particular, a clustering
with <img src="Evaluation%20of%20clustering_files/img1411.png" alt="$K=N$" width="51" height="32" border="0" align="MIDDLE"> one-document clusters has maximum MI.  So MI has
the same problem as purity: it does not penalize large
cardinalities and thus does not formalize our bias that,
other things being equal, fewer clusters are better.

</p><p>
The normalization by the denominator <!-- MATH
 $[H(\Omega )+H(\mathbb{C}
)]/2$
 -->
<img src="Evaluation%20of%20clustering_files/img1412.png" alt="$[H(\Omega )+H(\mathbb{C}
)]/2$" width="131" height="33" border="0" align="MIDDLE"> in Equation&nbsp;<a href="#nmidef">183</a> fixes this problem since entropy
tends to increase with the number of clusters.  For example,
<img src="Evaluation%20of%20clustering_files/img1413.png" alt="$H(\Omega)$" width="45" height="33" border="0" align="MIDDLE"> reaches its maximum <img src="Evaluation%20of%20clustering_files/img1414.png" alt="$\log N$" width="42" height="31" border="0" align="MIDDLE"> for <img src="Evaluation%20of%20clustering_files/img1411.png" alt="$K=N$" width="51" height="32" border="0" align="MIDDLE">, which

</p><p>
ensures that NMI is low for <img src="Evaluation%20of%20clustering_files/img1411.png" alt="$K=N$" width="51" height="32" border="0" align="MIDDLE">.  Because NMI is
normalized, we can use it to compare clusterings with
different numbers of clusters. The particular form of the
denominator is chosen because  <!-- MATH
 $[H(\Omega )+H(\mathbb{C}
)]/2$
 -->
<img src="Evaluation%20of%20clustering_files/img1412.png" alt="$[H(\Omega )+H(\mathbb{C}
)]/2$" width="131" height="33" border="0" align="MIDDLE"> is a tight upper bound on <!-- MATH
 $I(\Omega; \mathbb{C})$
 -->
<img src="Evaluation%20of%20clustering_files/img1409.png" alt="$I( \Omega ; \mathbb{C} )$" width="57" height="33" border="0" align="MIDDLE"> (Exercise <a href="https://nlp.stanford.edu/IR-book/html/htmledition/exercises-3.html#ex:nmibound">16.7</a> ). Thus,
NMI is always a number between 0 and 1.

</p><p>
An alternative to this information-theoretic interpretation
of clustering
is to view it as a series of decisions, one for each of
the 
<img src="Evaluation%20of%20clustering_files/img1415.png" alt="$N(N-1)/2$" width="91" height="33" border="0" align="MIDDLE">
pairs of documents in the collection. We
want to assign 
two
documents to the same cluster if and only if they are similar.
A true positive (TP) decision assigns two similar documents to
the same cluster, a true negative (TN) decision assigns two
dissimilar documents to different clusters.
There are two types of errors we can commit.
A <a name="24372"></a>  
(FP) decision
assigns two dissimilar documents to the same cluster. A
<a name="24374"></a>   
(FN) decision assigns two similar documents to
different clusters. 
The <a name="24376"></a> <i>Rand index</i>  
(<a name="24378"></a>  ) measures the percentage of decisions that
are correct.  That is, it is simply accuracy (Section <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html#sec:measuresperf">8.3</a> ,
page <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html#p:accuracy">8.3</a> ). 
</p><p></p>
<div align="CENTER">
<!-- MATH
 \begin{eqnarray*}
\mbox{RI} = \frac{\mbox{TP}+\mbox{TN}}{\mbox{TP}+\mbox{FP}+\mbox{FN}+\mbox{TN}}
\end{eqnarray*}
 -->
<img src="Evaluation%20of%20clustering_files/img1416.png" alt="\begin{eqnarray*}
\mbox{RI} = \frac{\mbox{TP}+\mbox{TN}}{\mbox{TP}+\mbox{FP}+\mbox{FN}+\mbox{TN}}
\end{eqnarray*}" width="182" height="41" border="0"></div>
<br clear="ALL"><p></p> 

<p>
As an example, we compute RI for
Figure <a href="#fig:clustfg3">16.4</a> . We first compute <!-- MATH
 $\mbox{TP}+\mbox{FP}$
 -->
<img src="Evaluation%20of%20clustering_files/img1417.png" alt="$\mbox{TP}+\mbox{FP}$" width="61" height="32" border="0" align="MIDDLE">.
The three clusters
contain 6, 6, and 5 points, respectively, so the total
number of ``positives'' or pairs of documents
that are in the same cluster is:
<br>
</p><div align="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{TP}+\mbox{FP} = \left( \begin{array}{c} 6 \\2 \end{array} \right) +
\left( \begin{array}{c} 6 \\2 \end{array} \right) +
\left( \begin{array}{c} 5 \\2 \end{array} \right) = 40
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img1418.png" alt="\begin{displaymath}
\mbox{TP}+\mbox{FP} = \left( \begin{array}{c} 6 \\ 2 \end{ar...
...ght) +
\left( \begin{array}{c} 5 \\ 2 \end{array} \right) = 40
\end{displaymath}" width="300" height="45" border="0"></td>
<td width="10" align="RIGHT">
(188)</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>
Of these, the x pairs in cluster&nbsp;1, the o pairs in
cluster&nbsp;2, the <img src="Evaluation%20of%20clustering_files/img1419.png" alt="$\diamond$" width="13" height="14" border="0" align="BOTTOM"> pairs in cluster&nbsp;3, and the x pair in
cluster&nbsp;3 are true positives:
<br>
<div align="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{TP}= \left( \begin{array}{c} 5 \\2 \end{array} \right) +
\left( \begin{array}{c} 4 \\2 \end{array} \right) +
\left( \begin{array}{c} 3 \\2 \end{array} \right) +
\left( \begin{array}{c} 2 \\2 \end{array} \right) = 20
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td nowrap="nowrap" align="CENTER"><img src="Evaluation%20of%20clustering_files/img1420.png" alt="\begin{displaymath}
\mbox{TP}= \left( \begin{array}{c} 5 \\ 2 \end{array} \right...
...ght) +
\left( \begin{array}{c} 2 \\ 2 \end{array} \right) = 20
\end{displaymath}" width="330" height="45" border="0"></td>
<td width="10" align="RIGHT">
(189)</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>
Thus, <!-- MATH
 $\mbox{FP}=40-20=20$
 -->
<img src="Evaluation%20of%20clustering_files/img1421.png" alt="$\mbox{FP}=40-20=20$" width="134" height="32" border="0" align="MIDDLE">. 

<p>
<img src="Evaluation%20of%20clustering_files/img1422.png" alt="$\mbox{FN}$" width="26" height="32" border="0" align="MIDDLE"> and <img src="Evaluation%20of%20clustering_files/img1423.png" alt="$\mbox{TN}$" width="27" height="32" border="0" align="MIDDLE"> are computed similarly,
resulting in the following contingency table:
</p><blockquote>
<table cellpadding="3" border="1">
<tbody><tr><td colspan="1" align="CENTER">&nbsp;</td>
<td colspan="1" align="CENTER">Same cluster</td>
<td colspan="1" align="CENTER">Different clusters</td>
</tr>
<tr><td align="LEFT">Same class</td>
<td align="CENTER"><!-- MATH
 $\mbox{TP} = 20$
 -->
<img src="Evaluation%20of%20clustering_files/img1424.png" alt="$\mbox{TP} = 20 $" width="61" height="32" border="0" align="MIDDLE"></td>
<td align="CENTER"><!-- MATH
 $\mbox{FN} = 24$
 -->
<img src="Evaluation%20of%20clustering_files/img1425.png" alt="$\mbox{FN} = 24$" width="64" height="32" border="0" align="MIDDLE"></td>
</tr>
<tr><td align="LEFT">Different classes</td>
<td align="CENTER"><!-- MATH
 $\mbox{FP} = 20$
 -->
<img src="Evaluation%20of%20clustering_files/img1426.png" alt="$\mbox{FP} = 20$" width="60" height="32" border="0" align="MIDDLE"></td>
<td align="CENTER"><!-- MATH
 $\mbox{TN} = 72$
 -->
<img src="Evaluation%20of%20clustering_files/img1427.png" alt="$\mbox{TN} = 72$" width="65" height="32" border="0" align="MIDDLE"></td>
</tr>
</tbody></table>
</blockquote>
<img src="Evaluation%20of%20clustering_files/img1428.png" alt="$\mbox{RI}$" width="20" height="32" border="0" align="MIDDLE"> is then <!-- MATH
 $(20+72)/(20+20+24+72) \approx 0.68$
 -->
<img src="Evaluation%20of%20clustering_files/img1429.png" alt="$(20+72)/(20+20+24+72) \approx 0.68$" width="265" height="33" border="0" align="MIDDLE">.

<p>
The Rand index gives equal weight to false positives and false
negatives. Separating similar documents
is sometimes worse than putting pairs of dissimilar
documents in the same cluster. We can use the
<a name="24446"></a> <i>F&nbsp;measure</i> 
measuresperf to
penalize false negatives more strongly than false positives by selecting a value <img src="Evaluation%20of%20clustering_files/img527.png" alt="$\beta &gt; 1$" width="44" height="31" border="0" align="MIDDLE">, thus
giving more weight to recall.
</p><p></p>
<div align="CENTER">
<!-- MATH
 \begin{eqnarray*}
P = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}} \qquad
R = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FN}} \qquad
F_{\beta} = \frac{(\beta^2+1)PR}{\beta^2 P+R}
\end{eqnarray*}
 -->
<img src="Evaluation%20of%20clustering_files/img1430.png" alt="\begin{eqnarray*}
P = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}} \qquad
R = \frac{\mb...
...+\mbox{FN}} \qquad
F_{\beta} = \frac{(\beta^2+1)PR}{\beta^2 P+R}
\end{eqnarray*}" width="375" height="47" border="0"></div>
<br clear="ALL"><p></p>
Based on the numbers in the contingency table,
<!-- MATH
 $P= 20/40
= 0.5$
 -->
<img src="Evaluation%20of%20clustering_files/img1431.png" alt="$P= 20/40
= 0.5$" width="120" height="31" border="0" align="MIDDLE"> and <!-- MATH
 $R= 20/44 \approx 0.455$
 -->
<img src="Evaluation%20of%20clustering_files/img1432.png" alt="$R= 20/44 \approx 0.455$" width="137" height="31" border="0" align="MIDDLE">. 
This gives us <!-- MATH
 $F_1
\approx 0.48$
 -->
<img src="Evaluation%20of%20clustering_files/img1433.png" alt="$F_1
\approx 0.48$" width="69" height="32" border="0" align="MIDDLE"> for <img src="Evaluation%20of%20clustering_files/img521.png" alt="$\beta = 1$" width="44" height="31" border="0" align="MIDDLE"> and <!-- MATH
 $F_5 \approx 0.456$
 -->
<img src="Evaluation%20of%20clustering_files/img1434.png" alt="$F_5 \approx 0.456$" width="76" height="32" border="0" align="MIDDLE"> for <img src="Evaluation%20of%20clustering_files/img529.png" alt="$\beta=5$" width="44" height="31" border="0" align="MIDDLE">.
In information retrieval,
evaluating clustering with <img src="Evaluation%20of%20clustering_files/img1435.png" alt="$F$" width="14" height="32" border="0" align="MIDDLE"> has the
advantage that the measure is already familiar to the
research community.

<p>
<b>Exercises.</b>
</p><ul>
<li>Replace every point <img src="Evaluation%20of%20clustering_files/img354.png" alt="$d$" width="12" height="31" border="0" align="MIDDLE"> in Figure <a href="#fig:clustfg3">16.4</a>  with
two identical copies of <img src="Evaluation%20of%20clustering_files/img354.png" alt="$d$" width="12" height="31" border="0" align="MIDDLE"> in the same class.
(i) Is it less difficult, equally difficult or more
difficult to cluster this set of 34 points as opposed to the
17 points in Figure <a href="#fig:clustfg3">16.4</a> ? (ii)
Compute purity, NMI,
RI, and <img src="Evaluation%20of%20clustering_files/img1397.png" alt="$F_5$" width="19" height="32" border="0" align="MIDDLE"> for the clustering with 34 points.
Which  measures increase and which stay the same after doubling the number of
points? 
(iii) Given your assessment in (i) and the results in (ii),
which measures are best suited to compare the quality of the
two clusterings?

<p>
</p></li>
</ul>

<p>
</p><hr>
<!--Navigation Panel-->
<a name="tex2html4211" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">
<img alt="next" src="Evaluation%20of%20clustering_files/next.png" width="37" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4205" href="https://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html">
<img alt="up" src="Evaluation%20of%20clustering_files/up.png" width="26" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4199" href="https://nlp.stanford.edu/IR-book/html/htmledition/cardinality---the-number-of-clusters-1.html">
<img alt="previous" src="Evaluation%20of%20clustering_files/prev.png" width="63" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4207" href="https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html">
<img alt="contents" src="Evaluation%20of%20clustering_files/contents.png" width="65" height="24" border="0" align="BOTTOM"></a> 
<a name="tex2html4209" href="https://nlp.stanford.edu/IR-book/html/htmledition/index-1.html">
<img alt="index" src="Evaluation%20of%20clustering_files/index.png" width="43" height="24" border="0" align="BOTTOM"></a> 
<br>
<b> Next:</b> <a name="tex2html4212" href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html">K-means</a>
<b> Up:</b> <a name="tex2html4206" href="https://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html">Flat clustering</a>
<b> Previous:</b> <a name="tex2html4200" href="https://nlp.stanford.edu/IR-book/html/htmledition/cardinality---the-number-of-clusters-1.html">Cardinality - the number</a>
 &nbsp; <b>  <a name="tex2html4208" href="https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html">Contents</a></b> 
 &nbsp; <b>  <a name="tex2html4210" href="https://nlp.stanford.edu/IR-book/html/htmledition/index-1.html">Index</a></b> 
<!--End of Navigation Panel-->
<address>
Â© 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org/">PDF edition</a> of the book.<br>
2009-04-07
</address>


</body></html>